<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Measuring What Neural Networks Really Learn ‚Äî Liyu Zerihun</title>
    <meta name="description" content="A framework for estimating neural network intrinsic dimensionality through low-rank factorization and distillation. Research by Liyu Zerihun.">
    <meta name="theme-color" content="#061018">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Liyu Zerihun">
    <meta property="og:title" content="Measuring What Neural Networks Really Learn ‚Äî Liyu Zerihun">
    <meta property="og:description" content="A framework for estimating neural network intrinsic dimensionality through low-rank factorization and distillation.">
    <meta property="og:url" content="https://LiyuZer.github.io/effective_rank.html">
    <meta property="og:image" content="https://LiyuZer.github.io/personal_img.jpg">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Measuring What Neural Networks Really Learn ‚Äî Liyu Zerihun">
    <meta name="twitter:description" content="A framework for estimating neural network intrinsic dimensionality through low-rank factorization and distillation.">
    <meta name="twitter:image" content="https://LiyuZer.github.io/personal_img.jpg">
    <link rel="icon" href="favicon.svg" type="image/svg+xml">
    <style>
        body {
            background-color: #22272a;
            color: #e2e4e6;
            font-family: 'Inter', Arial, sans-serif;
            margin: 0;
            padding: 0;
            display: flex;
            flex-direction: column;
            align-items: center;
        }
        .container {
            max-width: 750px;
            width: 100%;
            padding: 40px 24px 24px 24px;
            box-sizing: border-box;
        }
        h1, h2 {
            color: #85b8cb;
            font-weight: 600;
        }
        h1 {
            margin-bottom: 18px;
        }
        h2 {
            margin-top: 30px;
            margin-bottom: 12px;
            font-size: 1.15em;
        }
        p {
            line-height: 1.65;
            text-align: left;
        }
        .highlight-box {
            background: #272c30;
            border-left: 4px solid #85b8cb;
            padding: 14px 18px;
            margin: 18px 0;
            border-radius: 6px;
        }
        .links {
            display: flex;
            gap: 14px;
            margin: 20px 0;
            flex-wrap: wrap;
        }
        .links a {
            padding: 10px 16px;
            background: #272c30;
            border: 1px solid #85b8cb;
            border-radius: 8px;
            color: #6eb1d6;
            text-decoration: none;
            transition: all 0.2s;
        }
        .links a:hover {
            background: #32383d;
            border-color: #b3e5fc;
            color: #b3e5fc;
        }
        .footer {
            margin-top: 40px;
            font-size: 14px;
            color: #889097;
            text-align: center;
            font-style: italic;
        }
        code {
            background: #1a1f23;
            padding: 2px 6px;
            border-radius: 4px;
            color: #85b8cb;
            font-family: monospace;
        }
    </style>
    <link rel="stylesheet" href="theme.css">
</head>
<body>
<header class="site-header">
  <div class="inner">
    <div class="brand"><a href="index.html" style="border:0">Liyu Zerihun</a></div>
    <nav class="nav">
      <a href="index.html">Home</a>
      <a href="index.html#projects">Projects</a>
      <a href="writing.html">Writing</a>
      <a href="al.html">Ideas</a>
      <a href="goals.html">Goals</a>
      <a href="index.html#about">About</a>
    </nav>
  </div>
</header>
    <div class="container">
        <h1>Measuring What Neural Networks Really Learn</h1>
        <p style="font-style: italic; color: #96a4ae;">Research paper: Estimating the Effective Rank of Vision Transformers via Low-Rank Factorization</p>

        <h2>Motivation</h2>
        <p>
            Neural networks are typically over-parameterized relative to the complexity of the functions they learn. A Vision Transformer might have millions of parameters, but it's not clear how many of those parameters are actually necessary for the learned behavior. Put differently: <strong>what is the intrinsic dimensionality of a trained network?</strong>
        </p>

        <p>
            This question matters for practical reasons (compression, efficiency) and theoretical ones (understanding generalization and capacity). While we have well-developed notions of effective rank for matrices, extending this to neural networks‚Äîwhich consist of many matrices arranged in complex ways‚Äîis less obvious.
        </p>

        <h2>Background: Effective Rank of Matrices</h2>
        <p>
            The concept of effective rank is well-established in linear algebra and matrix theory. For a given matrix, the effective rank estimates how many singular values meaningfully contribute to its structure, as opposed to just counting non-zero singular values. Various definitions exist, often based on entropy measures or thresholding the singular value spectrum.
        </p>

        <p>
            However, applying this concept to neural networks is not straightforward. A trained network doesn't have a single weight matrix‚Äîit has many, arranged in complex architectures. More importantly, what matters isn't just the rank of individual weight matrices, but the <em>representational capacity</em> of the learned functions they compute. This raises a question: can we estimate how many dimensions a neural network actually uses in a way that reflects its learned behavior, not just its weight matrices?
        </p>

        <h2>An Empirical Approach</h2>
        <p>
            The method I explore in this paper is fairly straightforward:
        </p>

        <div class="highlight-box">
            <strong>The Process:</strong>
            <ol style="margin: 8px 0; padding-left: 20px;">
                <li>Train a full-rank "teacher" network to convergence</li>
                <li>Factorize its weight matrices at multiple ranks (r = 1, 2, 4, 8, ...)</li>
                <li>Train each low-rank "student" via knowledge distillation</li>
                <li>Measure performance as a function of rank</li>
            </ol>
        </div>

        <p>
            By systematically varying the rank and observing how performance changes, we can probe where the meaningful capacity lives. The approach treats effective rank as an empirical property measured through distillation, rather than something computed directly from weight matrices.
        </p>

        <h2>Effective Rank as a Region</h2>
        <p>
            One choice I made was to treat effective rank as a range rather than a single number. The idea is to identify the smallest contiguous set of ranks where a student can achieve 85-95% of the teacher's accuracy. This seems more realistic than pinpointing a single threshold, since the transition from insufficient to sufficient capacity is gradual.
        </p>

        <p>
            To make the estimates more stable, I fit the accuracy vs. rank curve with a monotone PCHIP interpolant. I also compute what I call an "effective knee"‚Äîthe rank that maximizes perpendicular distance between the smoothed curve and its endpoint secant. This is just a way to identify where improvements start to saturate.
        </p>

        <h2>Results on Vision Transformers</h2>
        <p>
            On Vision Transformers, the method shows that substantial compression is possible. Through systematic low-rank factorization and distillation, I was able to reduce parameters by roughly 11√ó while retaining about 94.7% of the teacher's accuracy. This suggests that these models, despite having millions of parameters, operate in considerably lower-dimensional spaces than their nominal capacity.
        </p>

        <p>
            The framework is automated and doesn't require manual tuning for each architecture. This makes it possible to systematically compare intrinsic dimensionality across different models, datasets, and training conditions, though more work is needed to understand how these measurements relate to generalization and other properties.
        </p>

        <h2>Why This Might Be Useful</h2>
        <p>
            Understanding intrinsic dimensionality could help address several questions in deep learning:
        </p>

        <ul style="line-height: 1.7;">
            <li><strong>Compression:</strong> Identifying which dimensions matter could guide better compression strategies beyond simple pruning.</li>
            <li><strong>Generalization:</strong> The relationship between effective rank and generalization is still unclear, but measuring both could help us understand this connection.</li>
            <li><strong>Transfer Learning:</strong> If different tasks share low-rank subspaces, this could inform how we think about transfer and multitask learning.</li>
            <li><strong>Interpretability:</strong> Focusing on the dimensions that carry most of the information might make interpretation more tractable.</li>
        </ul>

        <h2>Limitations and Future Work</h2>
        <p>
            This work has several limitations. The method requires training multiple student networks, which is computationally expensive. The choice of distillation objective and training procedure can affect the results. And most importantly, "effective rank" as measured here is just one proxy for intrinsic dimensionality‚Äîthere are likely other useful ways to measure this.
        </p>

        <p>
            The framework is also limited to architectures where low-rank factorization makes sense. Extending this to other types of structure (sparsity, quantization, etc.) would be interesting but isn't covered by the current approach.
        </p>

        <div class="links">
            <a href="https://arxiv.org/abs/2512.00792" target="_blank">üìÑ Read the Paper</a>
            <a href="https://github.com/LiyuZer/Geometric_Distillation" target="_blank">üíª View Code & Experiments</a>
        </div>

        <div class="post-nav" style="display:flex; justify-content:space-between; gap:12px; margin-top:22px;">
            <a href="writing.html">‚Üê All Writing</a>
        </div>

        <div class="footer">&copy; 2025 Liyu Zerihun <span class="amh">(·àç·ãï ·ã∞·à™·àÅ·äï)</span></div>
    </div>
</body>
</html>
